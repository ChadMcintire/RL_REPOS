name: FQF

# ————————————————
# Learning rates
# ————————————————
lr: 1e-5 # online‐Q learning rate
fp_lr: 1.e-7 # fraction-proposal head LR (≈100× smaller)
adam_eps: 3e-4 # Adam ε (≈0.0003)

# ————————————————
# Network Architecture
# ————————————————
n_embedding: 64 # cosine feature dim
kappa: 1.0 # Huber loss threshold
N: 32 # number of quantiles
lmda: 0.01 # entropy-bonus weight on τ

# ————————————————
# Gradient clipping
# ————————————————
opt_max_norm: 5.0 # clip_norm for main network
fp_max_norm: 0.5 # clip_norm for fp head

# ————————————————
# Replay and tartget
# ————————————————
reward_scale: 0.2 # scales raw Atari rewards
target_support: 200 # ±Vmax for Bellman targets
is_weight_max: 5.0 # clamp IS weights to ≤5

# ————————————————
# E-greedy schedule
# ————————————————
eps_type: linear 
initial_eps: 1.0
min_exp_eps: 0.03
eps_decay_steps: 500000 # only used by linear schedule
decay_rate: 1e-6 # only used by exponential schedule

# Main 
lr_scheduler_main:
  type:       step        # “step” | “lambda” | “cosine”
  step_size:  200000      # for StepLR (or T_max for cosine)
  gamma:      0.9         # decay factor for StepLR
  lambda_factor: 0.9999   # if you use type=lambda
  eta_min:     0.0        # optional for cosine

lr_scheduler_fp:
  type: lambda # for the tiny fp-lr we usually do a slow exp decay
  step_size: 100000 
  gamma: 0.9 # for 
  lambda_factor: 0.999995  # multiply fp_lr by this each scheduler.step()
  eta_min: 0.0 

